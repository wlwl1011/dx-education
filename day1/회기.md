선형 회귀 및 경사하강법
1. 서론
데이터 분석에서 두 변수 사이의 관계를 이해하고 예측하는 것은 중요한 문제입니다. 선형 회귀(Linear Regression)는 이러한 관계를 모델링하는 기본적인 방법 중 하나입니다. 또한, 경사하강법(Gradient Descent)은 이러한 모델을 학습하는 데 자주 사용되는 최적화 알고리즘입니다. 이 문서에서는 선형 회귀와 경사하강법의 개념, 이를 구현하는 방법, 그리고 Scikit-learn을 사용한 간단한 예제를 소개합니다.

2. 선형 회귀 (Linear Regression)
선형 회귀는 독립 변수(x)와 종속 변수(y) 사이의 선형 관계를 모델링합니다. 이는 다음과 같은 수식으로 표현됩니다:

𝑦
=
𝑤
𝑥
+
𝑏
y=wx+b

여기서 
𝑤
w는 기울기, 
𝑏
b는 절편입니다.

그림: 선형 회귀 모델의 개념


코드: 데이터 읽기 및 생성
python
코드 복사
import numpy as np
import pandas as pd

data_home = 'D:\\'
lin_data = pd.read_csv(data_home + 'linear.csv')

x = lin_data['X'].to_numpy()
y_true = lin_data['Y'].to_numpy()

np.random.seed(0)
noise = np.random.normal(0, 1, len(x))
y = 5 * x + 50 * noise
설명
데이터 읽기: CSV 파일에서 데이터를 읽어옵니다.
x와 y 생성: DataFrame에서 X와 Y 열의 값을 NumPy 배열로 변환합니다.
노이즈 추가: y 값에 노이즈를 추가하여 실제 데이터를 생성합니다.
3. 평균제곱오차 (MSE, Mean Squared Error)
평균제곱오차는 예측 값과 실제 값 사이의 차이의 제곱을 평균 내어 오차를 나타냅니다. 이는 모델의 성능을 평가하는 데 사용됩니다.

코드: MSE 함수 정의
python
코드 복사
def mse(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)
설명
MSE 함수: 예측 값과 실제 값 사이의 평균제곱오차를 계산합니다.
4. 모델 함수 정의
모델 함수는 주어진 x 값에 대해 예측 값을 계산합니다. 이는 선형 회귀 모델의 식을 구현한 것입니다.

코드: 모델 함수 정의
python
코드 복사
def model(x, w, b):
    return w * x + b
설명
모델 함수: 주어진 x 값에 대해 예측 값을 계산합니다.
5. 경사하강법 (Gradient Descent)
경사하강법은 비용 함수의 최소값을 찾기 위해 사용하는 최적화 알고리즘입니다. 선형 회귀의 경우, 비용 함수는 예측 값과 실제 값 사이의 오차를 나타내며, 이를 최소화하는 것이 목표입니다.

그림: 경사하강법의 개념

코드: 경사하강법을 사용한 학습
python
코드 복사
# 경사하강법 파라미터 설정
learning_rate = 1e-4
n_iterations = 100

# 초기 파라미터 설정
w = 0.5
b = 0.5

# 경사하강법 학습
m = len(x)
for i in range(n_iterations):
    y_hat = model(x, w, b)
    dw = (2/m) * np.sum((y_hat - y) * x)
    db = (2/m) * np.sum(y_hat - y)
    w -= learning_rate * dw
    b -= learning_rate * db

# 최종 모델 출력 및 MSE 계산
y_hat = model(x, w, b)
final_mse = mse(y, y_hat)
print(f'Final w: {w}, Final b: {b}')
print(f'Final MSE: {final_mse}')
설명
경사하강법 파라미터 설정: 학습률(learning rate)과 반복 횟수를 설정합니다.
파라미터 초기화: 
𝑤
w와 
𝑏
b를 초기화합니다.
경사하강법 학습: 비용 함수의 기울기를 계산하여 
𝑤
w와 
𝑏
b를 업데이트합니다.
결과 출력: 학습된 모델의 파라미터와 MSE를 출력합니다.
6. 데이터와 학습된 모델 시각화
코드: 데이터와 학습된 모델 시각화
python
코드 복사
import matplotlib.pyplot as plt

plt.scatter(x, y, label='Data')
plt.plot(x, y_hat, color='red', label='Model: y = wx + b')
plt.xlabel('Input (x)')
plt.ylabel('Output (y)')
plt.title('Data and Learned Linear Model')
plt.legend()
plt.show()
설명
데이터 시각화: 원본 데이터와 학습된 모델을 시각화하여 플롯으로 표시합니다.
7. Scikit-learn을 사용한 선형 회귀
Scikit-learn 라이브러리를 통해 선형 회귀 모델을 학습하고 결과를 출력하는 코드입니다.

코드: Scikit-learn을 사용한 학습 및 시각화
python
코드 복사
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Step 1: 데이터 읽기
data_home = 'D:\\'
lin_data = pd.read_csv(data_home + 'linear.csv')

x = lin_data['X'].to_numpy().reshape(-1, 1)  # Scikit-learn은 2D 배열을 입력으로 받음
y_true = lin_data['Y'].to_numpy()

# Step 2: Scikit-learn 모델 학습
model = LinearRegression()
model.fit(x, y_true)

# 예측값 계산
y_pred = model.predict(x)

# MSE 계산
mse_value = mean_squared_error(y_true, y_pred)
print(f'Final w: {model.coef_[0]}, Final b: {model.intercept_}')
print(f'Final MSE: {mse_value}')

# Step 3: 데이터와 학습된 모델 시각화
plt.scatter(x, y_true, label='Data')
plt.plot(x, y_pred, color='red', label='Model: y = wx + b')
plt.xlabel('Input (x)')
plt.ylabel('Output (y)')
plt.title('Data and Learned Linear Model (Scikit-learn)')
plt.legend()
plt.show()
설명
Scikit-learn 모델 학습: Scikit-learn의 LinearRegression 모델을 사용하여 데이터를 학습합니다.
예측 값 계산: 학습된 모델을 사용하여 예측 값을 계산합니다.
MSE 계산: Scikit-learn의 mean_squared_error 함수를 사용하여 MSE를 계산합니다.
시각화: 원본 데이터와 학습된 모델을 시각화하여 플롯으로 표시합니다.