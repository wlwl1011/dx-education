ì„ í˜• íšŒê·€ ë° ê²½ì‚¬í•˜ê°•ë²•
1. ì„œë¡ 
ë°ì´í„° ë¶„ì„ì—ì„œ ë‘ ë³€ìˆ˜ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì´í•´í•˜ê³  ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ ì¤‘ìš”í•œ ë¬¸ì œì…ë‹ˆë‹¤. ì„ í˜• íšŒê·€(Linear Regression)ëŠ” ì´ëŸ¬í•œ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ê¸°ë³¸ì ì¸ ë°©ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ë˜í•œ, ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)ì€ ì´ëŸ¬í•œ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë° ìì£¼ ì‚¬ìš©ë˜ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ì´ ë¬¸ì„œì—ì„œëŠ” ì„ í˜• íšŒê·€ì™€ ê²½ì‚¬í•˜ê°•ë²•ì˜ ê°œë…, ì´ë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ë²•, ê·¸ë¦¬ê³  Scikit-learnì„ ì‚¬ìš©í•œ ê°„ë‹¨í•œ ì˜ˆì œë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.

2. ì„ í˜• íšŒê·€ (Linear Regression)
ì„ í˜• íšŒê·€ëŠ” ë…ë¦½ ë³€ìˆ˜(x)ì™€ ì¢…ì† ë³€ìˆ˜(y) ì‚¬ì´ì˜ ì„ í˜• ê´€ê³„ë¥¼ ëª¨ë¸ë§í•©ë‹ˆë‹¤. ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤:

ğ‘¦
=
ğ‘¤
ğ‘¥
+
ğ‘
y=wx+b

ì—¬ê¸°ì„œ 
ğ‘¤
wëŠ” ê¸°ìš¸ê¸°, 
ğ‘
bëŠ” ì ˆí¸ì…ë‹ˆë‹¤.

ê·¸ë¦¼: ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ ê°œë…


ì½”ë“œ: ë°ì´í„° ì½ê¸° ë° ìƒì„±
python
ì½”ë“œ ë³µì‚¬
import numpy as np
import pandas as pd

data_home = 'D:\\'
lin_data = pd.read_csv(data_home + 'linear.csv')

x = lin_data['X'].to_numpy()
y_true = lin_data['Y'].to_numpy()

np.random.seed(0)
noise = np.random.normal(0, 1, len(x))
y = 5 * x + 50 * noise
ì„¤ëª…
ë°ì´í„° ì½ê¸°: CSV íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤.
xì™€ y ìƒì„±: DataFrameì—ì„œ Xì™€ Y ì—´ì˜ ê°’ì„ NumPy ë°°ì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
ë…¸ì´ì¦ˆ ì¶”ê°€: y ê°’ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ ì‹¤ì œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
3. í‰ê· ì œê³±ì˜¤ì°¨ (MSE, Mean Squared Error)
í‰ê· ì œê³±ì˜¤ì°¨ëŠ” ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ ì‚¬ì´ì˜ ì°¨ì´ì˜ ì œê³±ì„ í‰ê·  ë‚´ì–´ ì˜¤ì°¨ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

ì½”ë“œ: MSE í•¨ìˆ˜ ì •ì˜
python
ì½”ë“œ ë³µì‚¬
def mse(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)
ì„¤ëª…
MSE í•¨ìˆ˜: ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ ì‚¬ì´ì˜ í‰ê· ì œê³±ì˜¤ì°¨ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
4. ëª¨ë¸ í•¨ìˆ˜ ì •ì˜
ëª¨ë¸ í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ x ê°’ì— ëŒ€í•´ ì˜ˆì¸¡ ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ëŠ” ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ ì‹ì„ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤.

ì½”ë“œ: ëª¨ë¸ í•¨ìˆ˜ ì •ì˜
python
ì½”ë“œ ë³µì‚¬
def model(x, w, b):
    return w * x + b
ì„¤ëª…
ëª¨ë¸ í•¨ìˆ˜: ì£¼ì–´ì§„ x ê°’ì— ëŒ€í•´ ì˜ˆì¸¡ ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
5. ê²½ì‚¬í•˜ê°•ë²• (Gradient Descent)
ê²½ì‚¬í•˜ê°•ë²•ì€ ë¹„ìš© í•¨ìˆ˜ì˜ ìµœì†Œê°’ì„ ì°¾ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ì„ í˜• íšŒê·€ì˜ ê²½ìš°, ë¹„ìš© í•¨ìˆ˜ëŠ” ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ ì‚¬ì´ì˜ ì˜¤ì°¨ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì´ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.

ê·¸ë¦¼: ê²½ì‚¬í•˜ê°•ë²•ì˜ ê°œë…

ì½”ë“œ: ê²½ì‚¬í•˜ê°•ë²•ì„ ì‚¬ìš©í•œ í•™ìŠµ
python
ì½”ë“œ ë³µì‚¬
# ê²½ì‚¬í•˜ê°•ë²• íŒŒë¼ë¯¸í„° ì„¤ì •
learning_rate = 1e-4
n_iterations = 100

# ì´ˆê¸° íŒŒë¼ë¯¸í„° ì„¤ì •
w = 0.5
b = 0.5

# ê²½ì‚¬í•˜ê°•ë²• í•™ìŠµ
m = len(x)
for i in range(n_iterations):
    y_hat = model(x, w, b)
    dw = (2/m) * np.sum((y_hat - y) * x)
    db = (2/m) * np.sum(y_hat - y)
    w -= learning_rate * dw
    b -= learning_rate * db

# ìµœì¢… ëª¨ë¸ ì¶œë ¥ ë° MSE ê³„ì‚°
y_hat = model(x, w, b)
final_mse = mse(y, y_hat)
print(f'Final w: {w}, Final b: {b}')
print(f'Final MSE: {final_mse}')
ì„¤ëª…
ê²½ì‚¬í•˜ê°•ë²• íŒŒë¼ë¯¸í„° ì„¤ì •: í•™ìŠµë¥ (learning rate)ê³¼ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”: 
ğ‘¤
wì™€ 
ğ‘
bë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
ê²½ì‚¬í•˜ê°•ë²• í•™ìŠµ: ë¹„ìš© í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ì—¬ 
ğ‘¤
wì™€ 
ğ‘
bë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
ê²°ê³¼ ì¶œë ¥: í•™ìŠµëœ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ì™€ MSEë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
6. ë°ì´í„°ì™€ í•™ìŠµëœ ëª¨ë¸ ì‹œê°í™”
ì½”ë“œ: ë°ì´í„°ì™€ í•™ìŠµëœ ëª¨ë¸ ì‹œê°í™”
python
ì½”ë“œ ë³µì‚¬
import matplotlib.pyplot as plt

plt.scatter(x, y, label='Data')
plt.plot(x, y_hat, color='red', label='Model: y = wx + b')
plt.xlabel('Input (x)')
plt.ylabel('Output (y)')
plt.title('Data and Learned Linear Model')
plt.legend()
plt.show()
ì„¤ëª…
ë°ì´í„° ì‹œê°í™”: ì›ë³¸ ë°ì´í„°ì™€ í•™ìŠµëœ ëª¨ë¸ì„ ì‹œê°í™”í•˜ì—¬ í”Œë¡¯ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
7. Scikit-learnì„ ì‚¬ìš©í•œ ì„ í˜• íšŒê·€
Scikit-learn ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ì„ í˜• íšŒê·€ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.

ì½”ë“œ: Scikit-learnì„ ì‚¬ìš©í•œ í•™ìŠµ ë° ì‹œê°í™”
python
ì½”ë“œ ë³µì‚¬
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Step 1: ë°ì´í„° ì½ê¸°
data_home = 'D:\\'
lin_data = pd.read_csv(data_home + 'linear.csv')

x = lin_data['X'].to_numpy().reshape(-1, 1)  # Scikit-learnì€ 2D ë°°ì—´ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ
y_true = lin_data['Y'].to_numpy()

# Step 2: Scikit-learn ëª¨ë¸ í•™ìŠµ
model = LinearRegression()
model.fit(x, y_true)

# ì˜ˆì¸¡ê°’ ê³„ì‚°
y_pred = model.predict(x)

# MSE ê³„ì‚°
mse_value = mean_squared_error(y_true, y_pred)
print(f'Final w: {model.coef_[0]}, Final b: {model.intercept_}')
print(f'Final MSE: {mse_value}')

# Step 3: ë°ì´í„°ì™€ í•™ìŠµëœ ëª¨ë¸ ì‹œê°í™”
plt.scatter(x, y_true, label='Data')
plt.plot(x, y_pred, color='red', label='Model: y = wx + b')
plt.xlabel('Input (x)')
plt.ylabel('Output (y)')
plt.title('Data and Learned Linear Model (Scikit-learn)')
plt.legend()
plt.show()
ì„¤ëª…
Scikit-learn ëª¨ë¸ í•™ìŠµ: Scikit-learnì˜ LinearRegression ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
ì˜ˆì¸¡ ê°’ ê³„ì‚°: í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
MSE ê³„ì‚°: Scikit-learnì˜ mean_squared_error í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ MSEë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
ì‹œê°í™”: ì›ë³¸ ë°ì´í„°ì™€ í•™ìŠµëœ ëª¨ë¸ì„ ì‹œê°í™”í•˜ì—¬ í”Œë¡¯ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.